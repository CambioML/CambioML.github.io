---
title: 'KDD 2024: Samtale med Amazon'
date: '2025-01-29'
keywords: 'KDD 2024,Store språkmodeller,LLM,Hentingsforsterket generering,RAG,LLM finjustering,Amazon,domenespesifikk AI,maskinlæring,konferanse'
image: '/images/solutions/kdd-2024-cover.jpeg'
---

![KDD 2024 Konferanse](/images/solutions/kdd-2024-cover.jpeg)
_Rachel Hu presenterer på KDD 2024 konferansen_

På KDD 2024 konferansen presenterte [Rachel Hu](https://www.linkedin.com/in/rachelsonghu/), medgründer og administrerende direktør i CambioML, en omfattende tutorial om optimalisering av Store Språkmodeller (LLM) for domenespesifikke applikasjoner, sammen med medpresentatører [José Cassio dos Santos Junior](https://www.linkedin.com/in/jcassiojr/) (Amazon), [Richard Song](https://www.linkedin.com/in/renchu-richard-song-a4099247/) (Epsilla), og [Yunfei Bai](https://www.linkedin.com/in/yunfei-felix-bai-909b861/) (Amazon). Økten ga dyp innsikt i to kritiske teknikker: Hentingsforsterket Generering (RAG) og LLM Finjustering. Disse metodene er essensielle for å forbedre ytelsen til LLM-er innen spesialiserte felt, og lar utviklere lage mer effektive og nøyaktige modeller tilpasset spesifikke oppgaver.

## Forstå RAG: Utvidelse av LLM-funksjoner

Hentingsforsterket Generering (RAG) er en kraftig tilnærming som utvider funksjonene til LLM-er ved å integrere eksterne kunnskapsbaser. Denne teknikken gjør det mulig for LLM-er å generere svar basert på spesifikk domenekunnskap uten å kreve omfattende omtrening. RAG er spesielt gunstig for organisasjoner som trenger å utnytte interne kunnskapsbaser eller andre spesialiserte ressurser, og gir en måte å forbedre LLM-ytelsen på en kostnadseffektiv og tidsbesparende måte.

## Finjustering: Tilpasning av modeller for presisjon

LLM Finjustering innebærer justering av modellens vekter ved hjelp av domenespesifikke data, noe som gjør at modellen systematisk kan lære ny, omfattende kunnskap som ikke var inkludert under forhåndsopplæringen. Denne tilnærmingen er essensiell for oppgaver som krever høy grad av nøyaktighet og er spesielt effektiv i domener der generelle modeller ikke strekker til. Finjustering kan forvandle en LLM til et høyt spesialisert verktøy, i stand til å utføre komplekse, domenespesifikke oppgaver med presisjon.

![Rachel Hu presenterer på KDD](/images/solutions/kdd-2024-rachel.jpeg)

## Kombinere RAG og Finjustering for optimale resultater

Tutorialen utforsket hvordan kombinasjonen av RAG og Finjustering kan skape en robust arkitektur for LLM-applikasjoner. Ved å integrere disse to tilnærmingene kan utviklere bygge modeller som ikke bare får tilgang til den mest relevante eksterne informasjonen, men også lærer fra domenespesifikke data. Denne hybride tilnærmingen gjør det mulig å lage modeller som er både allsidige og svært nøyaktige, i stand til å håndtere et bredt spekter av domenespesifikke oppgaver, fra tekstgenerering til komplekse spørsmål-svar-scenarier.

## Praktiske laboratorier: Praktiske anvendelser av RAG og Finjustering

En betydelig del av Rachels tutorial var viet til praktiske laboratorier, der deltakerne utforsket avanserte teknikker for å optimalisere RAG og Finjusterte LLM-arkitekturer. Laboratoriene dekket en rekke emner, inkludert:

- **Avanserte RAG-teknikker**: Multi-fase optimaliseringsstrategier ble demonstrert for å forbedre nøyaktigheten og relevansen av RAG-utdata. Dette inkluderte pre-henting, henting og post-henting optimalisering, samt innovativ bruk av kunnskapsgrafer og multi-dokumentanalyse for mer nyansert resonnement.

- **Finjustering av LLM-er**: Deltakerne deltok i finjustering av en liten LLM ved hjelp av domenespesifikke datasett. Laboratoriet fremhevet den kontinuerlige finjusteringsprosessen, som integrerte både menneskelig og AI-tilbakemelding for å oppnå overlegen ytelse i spesialiserte oppgaver.

- **Benchmarking og evaluering**: Det siste laboratoriet fokuserte på å sammenligne ytelsen til RAG, Finjustering, og deres kombinerte tilnærming på tvers av ulike oppgaver. Dette inkluderte en detaljert ROI-analyse for å hjelpe utviklere med å velge den mest kostnadseffektive og effektive metoden for deres spesifikke behov.

![KDD 2024 Laboratorier](/images/solutions/kdd-2024-labs.jpg)

## Beste praksis for domenespesifikk LLM-utvikling

Tutorialen ble avsluttet med et sett med beste praksis for implementering av RAG og Finjustering i virkelige applikasjoner. Ved å understreke viktigheten av å forstå avveiningene mellom RAGs fleksibilitet og Finjusterings presisjon, ble deltakerne oppfordret til å engasjere seg i kontinuerlig eksperimentering og benchmarking. Denne tilnærmingen sikrer at ytelses- og kostnadseffektivitetskriterier blir oppfylt, noe som lar utviklere optimalisere LLM-arkitekturen sin for domenespesifikke oppgaver effektivt.

For en mer detaljert oversikt over innholdet i tutorialen og de praktiske laboratoriene, vennligst se [dette papiret](https://dl.acm.org/doi/pdf/10.1145/3637528.3671445) og [denne presentasjonen](https://docs.google.com/presentation/d/18PJctnI-KbABE1El_AifjN_7eoHatuaoN8-2q57xpSw/edit#slide=id.g2f5cc21ff85_5_1096).
